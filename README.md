# Estimating Object Depth in Images: A Combined Stereo Vision and Deep Learning Approach

## 1. Introduction

This project tackles the challenge of estimating object depth (distance from camera to object) in images by combining traditional stereo vision techniques with the power of deep learning-based object detection.

### 1.1 Problem Definition

#### 1.1.1 Research Objective
Our research focuses on accurately estimating object depth in images by combining:

- **Depth Estimation:** Determining the distance of objects from the camera.
- **Object Detection:** Identifying and localizing objects within an image.

#### 1.1.2 Input Data

- **Stereo Images:** Two images captured simultaneously from slightly different viewpoints (like human eyes), used as input for traditional stereo vision algorithms.
- **Single Images:** Extracted from stereo pairs, used as input for deep learning models.

#### 1.1.3 Output Data

- **Depth Map:** A representation of the scene where each pixel value corresponds to its distance from the camera. Generated by both traditional and deep learning methods.
- **Object Information:**
   - **Bounding Box:** A rectangle that encloses the detected object, providing its location in the image.
   - **Depth Values:** Calculated within the bounding box:
      - **Min:** Minimum depth value (closest point of the object).
      - **Mean:** Average depth value of the object.
      - **Median:** Median depth value, robust to outliers.
  <img src="/output/frame34.png" alt="Example Image 1"/>

**Method 1: Stereo Vision**

<p align="center">
  <img src="/output/width_396.webp" alt="Example Image 2" width="396"/>
</p>
1. **Matching Pixels:** The algorithm identifies corresponding points (like corners, wheels) representing the same location on the bus in both images.
2. **Disparity Calculation:** By comparing the position difference of these points in the two images, disparity is calculated, representing the shift in object position when viewed from different angles.
3. **Disparity to Depth Conversion:** Disparity, combined with camera parameters like baseline (distance between cameras), is used to calculate the distance of each point on the bus from the cameras, resulting in a depth map.
4. **Object Identification and Depth Calculation:**
   - The algorithm detects and outlines the bus in the depth map, creating a bounding box to indicate its location.
   - Depth values (minimum, average, median) are calculated within the bounding box to understand the bus's distance.

**Method 2: Deep Learning**

1. **Deep Learning Model:** A model (e.g., U-Net, MiDaS) is trained to predict depth directly from a single image.
2. **Single Image Input:** The model takes the image of the bus as input.
3. **Depth Map Output:** The model analyzes the image and predicts depth for each pixel, creating a depth map.

**Results:** Both stereo vision and deep learning generate depth maps and provide object location (bounding box) and depth information, enabling us to understand the bus's 3D position and its distance from other objects.

## MiDaS + YOLOv8 Demo

![Application Demo](/output/out6%20(29).gif)

#### 1.1.5 Datasets Used

- **KITTI Vision Benchmark Suite:** Provides real-world driving scenes with stereo images, ground truth depth maps, camera information, and object annotations (https://www.cvlibs.net/datasets/kitti/).
- **DIODE:** Offers diverse indoor and outdoor scenes with high-resolution single images, depth maps, and segmentation masks, primarily used for training deep learning models (https://diode-dataset.org/).

## 2. Comparing Traditional and Deep Learning Methods

This chapter compares the performance of:

- **StereoSGBM (Traditional Stereo Vision):** Utilizes stereo images and relies on camera calibration for depth estimation.
- **MiDaS (Deep Learning):** Predicts depth from single images using a pre-trained model.

### 2.1 Performance Comparison

| Method       | Advantages                                                                          | Disadvantages                                                                                                                                                              |
|--------------|-----------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| StereoSGBM | - Easy setup with OpenCV. <br> - No model training required.                    | - Accuracy highly dependent on camera calibration quality. <br>-  Challenges with low-texture regions, occlusions, and varying lighting conditions.                   |
| MiDaS      | - High accuracy and good generalization. <br>- Pre-trained and easy to use. <br>- Fast processing speed. | - Computationally more expensive than traditional methods. <br>- Evaluation metrics may not fully capture depth map quality.                                          |                                       |

### 2.2 Detailed Analysis

#### 2.2.1 StereoSGBM

- **Advantages:**
   - **Simplicity and Ease of Implementation:**  Readily available in OpenCV, simplifying integration.
   - **No Training Required:**  Works directly with stereo images without the need for model training.

- **Disadvantages:**
   - **Dependence on Camera Calibration:** Accuracy heavily relies on precise camera calibration. Errors in calibration directly impact depth map accuracy.
   - **Handling Complexities:** Struggles with:
      - **Low-texture Regions:**  Difficulty finding accurate correspondences in areas with minimal detail.
      - **Occlusions:** Objects hidden in one image of the stereo pair lead to inaccuracies in the depth map. 
      - **Varying Lighting Conditions:** Changes in lighting between stereo images can affect performance.

#### 2.2.2 MiDaS

- **Advantages:**
   - **High Accuracy:**  Outperforms StereoSGBM, especially in challenging scenarios.
   - **Generalization:** Trained on diverse datasets, allowing it to perform well on a wide range of images.
   - **Ease of Use:**  Pre-trained models are readily available, eliminating the need for extensive training.
   - **Fast Processing:** Enables near real-time applications.

- **Disadvantages:**
   - **Computational Cost:**  Requires more computational resources compared to StereoSGBM, potentially limiting its use on resource-constrained devices.
   - **Evaluation Metrics:** Traditional metrics like MAE, RMSE, and SSIM might not fully reflect the quality of the depth map, especially considering potential discrepancies in the overall structure compared to ground truth data.

## 3. Conclusion

This research successfully implemented a system for object depth estimation, combining:

- **Traditional Stereo Vision (StereoSGBM)**
- **Deep Learning (U-Net, MiDaS)**
- **Object Detection (YOLOv8)**

Key findings:
 - MiDaS provided more accurate depth estimation compared to U-Net and StereoSGBM.
 - MiDaS also demonstrated faster processing speeds, making it suitable for real-time applications.


